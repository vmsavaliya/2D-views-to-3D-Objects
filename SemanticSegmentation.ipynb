{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the semantic segmentation model\n",
    "model = torch.hub.load('pytorch/vision:v0.5.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\"\"\"\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\"\"\"\n",
    "filename = '/home/sidroy/Downloads/chair_views/chair5.jpg'\n",
    "#filename = '/home/sidroy/Insight/projects/Pix2Vox/LargeDatasets/Pix3D/img/sofa/0128.jpg'\n",
    "segment_image(filename,padding=0,img_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "scale_percent = 517/img.shape[0]*100 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "print(scale_percent,'%')\n",
    "dim = (width, height)\n",
    "print(\"Dimensions of input to segmentation code {}\".format(dim))\n",
    "# resize image\n",
    "img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "cv2.imwrite('segmentation_image.png', img)\n",
    "filename = 'segmentation_image.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# Input image is opened in this line\n",
    "input_image = Image.open(filename)\n",
    "print(\"Shape of Input Image {}\".format(input_image.size))\n",
    "dim_input_image = len(input_image.size)\n",
    "\"\"\"\n",
    "if dim_input_image == 2:\n",
    "    input_image = input_image.resize(1,input_image.size[0],input_image.size[1])\n",
    "\"\"\"\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# preprocessed image after all the transformations\n",
    "input_tensor = preprocess(input_image)\n",
    "print(\"Input Tensor Size is {}\".format(input_tensor.shape))\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # output is the pytorch tensor output by the segmentation model\n",
    "    # it contains the probability of each pixel being ina certain class\n",
    "    output = model(input_batch)['out'][0]\n",
    "# output predictions is array of pixels with max probability of being in a class\n",
    "output_predictions = output.argmax(0)\n",
    "print('output shape is {}'.format(output.shape))\n",
    "print('output_predictions is of shape {}'.format(output_predictions.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a color pallette, selecting a color for each class\n",
    "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
    "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
    "colors = (colors % 255).numpy().astype(\"uint8\")\n",
    "\n",
    "# plot the semantic segmentation predictions of 21 classes in each color\n",
    "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
    "r.putpalette(colors)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.imshow(r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.imshow(  input_tensor.permute(1, 2, 0)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "mask = output_predictions.cpu()\n",
    "plt.imshow( mask )\n",
    "\n",
    "categories, counts = np.unique(mask, return_counts=True)\n",
    "if len(counts) == 1:\n",
    "    max_count_index = np.argmax(counts)+1\n",
    "else:\n",
    "    max_count_index = np.argmax(counts[1:])+1\n",
    "    \n",
    "category = categories[max_count_index]\n",
    "print(category)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "res = np.array(input_image)\n",
    "res[60, 60]\n",
    "b_channel, g_channel, r_channel = cv2.split(res)\n",
    "print(b_channel.shape)\n",
    "print(g_channel.shape)\n",
    "print(r_channel.shape)\n",
    "\n",
    "mask_array = np.asarray(mask)\n",
    "mask_array = np.where(mask!=category, 0, 255)\n",
    "mask_array = mask_array.astype(np.uint8)\n",
    "print( type(mask_array) ) \n",
    "print( type(b_channel)  )\n",
    "\n",
    "alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 50\n",
    "img_BGRA = cv2.merge((b_channel, g_channel, r_channel, mask_array))\n",
    "img_RGBA = cv2.merge(( r_channel, g_channel, b_channel, mask_array))\n",
    "\n",
    "image_width = img_RGBA.shape[1]\n",
    "image_height = img_RGBA.shape[0]\n",
    "scale_percent = 500/image_width*100\n",
    "print(scale_percent)\n",
    "width = int(img_RGBA.shape[1] * scale_percent / 100)\n",
    "height = int(img_RGBA.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "resized_RGBA = cv2.resize(img_RGBA, dim, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.imshow(img_BGRA)\n",
    "plt.show()\n",
    "cv2.imwrite('/home/sidroy/Insight/projects/Pix2Vox/datasets/DemoImage/car/car_subfolder/rendering/segmented.png', resized_RGBA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = 137\n",
    "extra_padding = 0\n",
    "im = resized_RGBA\n",
    "print(\"Read image size {}\".format(im.shape))\n",
    "\n",
    "old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "print(\"Old Size is {}\".format(old_size))\n",
    "\n",
    "ratio = float(desired_size)/max(old_size)\n",
    "new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "print(\"New Size is {}\".format(new_size))\n",
    "\n",
    "# new_size should be in (width, height) format\n",
    "\n",
    "im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "delta_w = desired_size - new_size[1]\n",
    "delta_h = desired_size - new_size[0]\n",
    "top, bottom = delta_h//2 + extra_padding, delta_h-(delta_h//2) + extra_padding\n",
    "left, right = delta_w//2 + extra_padding, delta_w-(delta_w//2) + extra_padding\n",
    "\n",
    "color = [0, 0, 0]\n",
    "new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "    value=color)\n",
    "new_im = cv2.resize(new_im, (desired_size,desired_size), interpolation = cv2.INTER_AREA)\n",
    "print('final size = {}'.format(new_im.shape))\n",
    "plt.imshow(new_im)\n",
    "\n",
    "cv2.imwrite('/home/sidroy/Insight/projects/Pix2Vox/datasets/DemoImage/car/car_subfolder/rendering/00.png', new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"load_images\", \"*.*\"))\n",
    "\n",
    "print(file_list)\n",
    "\n",
    "i = 0\n",
    "for file_path in file_list:\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
    "    img_name = '{:02d}'.format(i)\n",
    "    cv2.imwrite(img_name+'.png',img)\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"load_images\"\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), folder_name, \"*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(filename,padding,img_number):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    img = cv2.imread(filename, cv2.IMREAD_UNCHANGED)\n",
    "    scale_percent = 517/img.shape[0]*100 # percent of original size\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    print(scale_percent,'%')\n",
    "    dim = (width, height)\n",
    "    print(\"Dimensions of input to segmentation code {}\".format(dim))\n",
    "    # resize image\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    cv2.imwrite('segmentation_image.png', img)\n",
    "    filename = 'segmentation_image.png'\n",
    "    \n",
    "    # sample execution (requires torchvision)\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "    # Input image is opened in this line\n",
    "    input_image = Image.open(filename)\n",
    "    print(\"Shape of Input Image {}\".format(input_image.size))\n",
    "    dim_input_image = len(input_image.size)\n",
    "    \"\"\"\n",
    "    if dim_input_image == 2:\n",
    "        input_image = input_image.resize(1,input_image.size[0],input_image.size[1])\n",
    "    \"\"\"\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # preprocessed image after all the transformations\n",
    "    input_tensor = preprocess(input_image)\n",
    "    print(\"Input Tensor Size is {}\".format(input_tensor.shape))\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # output is the pytorch tensor output by the segmentation model\n",
    "        # it contains the probability of each pixel being ina certain class\n",
    "        output = model(input_batch)['out'][0]\n",
    "    # output predictions is array of pixels with max probability of being in a class\n",
    "    output_predictions = output.argmax(0)\n",
    "    print('output shape is {}'.format(output.shape))\n",
    "    print('output_predictions is of shape {}'.format(output_predictions.shape))\n",
    "\n",
    "    %matplotlib notebook\n",
    "    mask = output_predictions.cpu()\n",
    "    plt.imshow( mask )\n",
    "\n",
    "    categories, counts = np.unique(mask, return_counts=True)\n",
    "    if len(counts) == 1:\n",
    "        max_count_index = np.argmax(counts)+1\n",
    "    else:\n",
    "        max_count_index = np.argmax(counts[1:])+1\n",
    "\n",
    "    category = categories[max_count_index]\n",
    "    print(category)\n",
    "\n",
    "    import cv2\n",
    "    res = np.array(input_image)\n",
    "    res[60, 60]\n",
    "    b_channel, g_channel, r_channel = cv2.split(res)\n",
    "    print(b_channel.shape)\n",
    "    print(g_channel.shape)\n",
    "    print(r_channel.shape)\n",
    "\n",
    "    mask_array = np.asarray(mask)\n",
    "    mask_array = np.where(mask!=category, 0, 255)\n",
    "    mask_array = mask_array.astype(np.uint8)\n",
    "    print( type(mask_array) ) \n",
    "    print( type(b_channel)  )\n",
    "\n",
    "    alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 50\n",
    "    img_BGRA = cv2.merge((b_channel, g_channel, r_channel, mask_array))\n",
    "    img_RGBA = cv2.merge(( r_channel, g_channel, b_channel, mask_array))\n",
    "\n",
    "    image_width = img_RGBA.shape[1]\n",
    "    image_height = img_RGBA.shape[0]\n",
    "    scale_percent = 137/image_width*100\n",
    "    print(scale_percent)\n",
    "    width = int(img_RGBA.shape[1] * scale_percent / 100)\n",
    "    height = int(img_RGBA.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    resized_RGBA = cv2.resize(img_RGBA, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    desired_size = 137\n",
    "    extra_padding = padding\n",
    "    im = resized_RGBA\n",
    "    print(\"Read image size {}\".format(im.shape))\n",
    "\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    print(\"Old Size is {}\".format(old_size))\n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    print(\"New Size is {}\".format(new_size))\n",
    "\n",
    "    # new_size should be in (width, height) format\n",
    "\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "\n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2 + extra_padding, delta_h-(delta_h//2) + extra_padding\n",
    "    left, right = delta_w//2 + extra_padding, delta_w-(delta_w//2) + extra_padding\n",
    "\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "        value=color)\n",
    "    new_im = cv2.resize(new_im, (desired_size,desired_size), interpolation = cv2.INTER_AREA)\n",
    "    print('final size = {}'.format(new_im.shape))\n",
    "    plt.imshow(new_im)\n",
    "\n",
    "    cv2.imwrite('/home/sidroy/Insight/projects/Pix2Vox/datasets/DemoImage/car/car_subfolder/rendering/{:02d}.png'.format(img_number), new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
